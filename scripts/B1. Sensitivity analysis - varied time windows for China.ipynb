{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b86d65-f152-4e0e-b63a-3e694e190414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "recalc_everything = True\n",
    "\n",
    "import altair as alt\n",
    "print(alt.__version__)\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.renderers.enable('default')\n",
    "import altair_saver\n",
    "import selenium\n",
    "print(selenium.__version__)\n",
    "\n",
    "save_figures = True\n",
    "import vl_convert as vlc\n",
    "def save_chart(chart_to_save, filename):\n",
    "    svg_str = vlc.vegalite_to_svg(chart_to_save.to_json())\n",
    "    with open(filename, \"wt\") as f:\n",
    "        f.write(svg_str)\n",
    "\n",
    "import termcolor\n",
    "import io\n",
    "import os\n",
    "\n",
    "mainstandirname = '../../../Taiwan_Backup/Monkeypox_2022'\n",
    "os.makedirs(mainstandirname, exist_ok=True)\n",
    "\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "%matplotlib inline\n",
    "%config matplotlib_inline.matplotlib_formats = 'retina'\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# chinese font\n",
    "from matplotlib import font_manager\n",
    "fontP = font_manager.FontProperties(fname=\"./NotoSerifTC-Regular.otf\")\n",
    "fontP.set_size(10)\n",
    "\n",
    "import pathlib\n",
    "import platform\n",
    "\n",
    "import cmdstanpy as cmdstan\n",
    "import arviz as az\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='arviz')\n",
    "standistribdir = '../../../../CmdStan'\n",
    "cmdstan.set_cmdstan_path(standistribdir[3:] if platform.system()=='Darwin' else standistribdir)\n",
    "\n",
    "func_dict = {\"q2.5\": lambda x: np.percentile(x, 2.5),\n",
    "             \"q25\": lambda x: np.percentile(x, 25),\n",
    "             \"median\": lambda x: np.percentile(x, 50),\n",
    "             \"q75\": lambda x: np.percentile(x, 75),\n",
    "             \"q97.5\": lambda x: np.percentile(x, 97.5)}\n",
    "\n",
    "def get_stats(cmdstan_data, varnames, round_to_=5):\n",
    "    # include mean and hdi\n",
    "    stats = az.summary(cmdstan_data, round_to=round_to_, var_names=varnames, hdi_prob=0.95).loc[:, ['mean','hdi_2.5%','hdi_97.5%','ess_bulk','ess_tail','r_hat']].reset_index().rename(columns={'index':'var', 'hdi_2.5%':'hdi2.5', 'hdi_97.5%':'hdi97.5'})\n",
    "    stats = az.summary(cmdstan_data, round_to=round_to_, var_names=varnames, hdi_prob=0.50).loc[:, ['hdi_25%','hdi_75%']].reset_index().rename(columns={'index':'var', 'hdi_25%':'hdi25', 'hdi_75%':'hdi75'}).\\\n",
    "        merge(stats, left_on='var', right_on='var')\n",
    "    # include percentiles\n",
    "    stats = az.summary(cmdstan_data, round_to=round_to_, var_names=varnames, stat_funcs=func_dict, extend=False).reset_index().rename(columns={'index': 'var'}).merge(stats, left_on='var', right_on='var')\n",
    "    stats['time'] = stats['var'].apply(lambda st: st[st.find(\"[\")+1:st.find(\"]\")])\n",
    "    stats['time'] = ['NA' if \"[\" not in y else int(x)+1 for x,y in zip(stats['time'],stats['var'])]\n",
    "    stats['var'] = stats['var'].apply(lambda st: st[:st.find(\"[\")] if \"[\" in st else st)\n",
    "    return stats.loc[:,['var','time','mean','hdi2.5','hdi25','hdi75','hdi97.5','q2.5','q25','median','q75','q97.5','ess_bulk','ess_tail','r_hat']]\n",
    "\n",
    "def get_stats_2d(cmdstan_data, varnames, rounding=2):\n",
    "    # include mean and hpd\n",
    "    stats = az.summary(cmdstan_data, var_names=varnames, hdi_prob=0.95, round_to=rounding).loc[:, ['mean','hdi_2.5%','hdi_97.5%','ess_bulk','ess_tail','r_hat']].reset_index().rename(columns={'index':'var', 'hdi_2.5%':'hdi2.5', 'hdi_97.5%':'hdi97.5'})\n",
    "    stats = az.summary(cmdstan_data, var_names=varnames, hdi_prob=0.50, round_to=rounding).loc[:, ['hdi_25%','hdi_75%']].reset_index().rename(columns={'index':'var', 'hdi_25%':'hdi25', 'hdi_75%':'hdi75'}).\\\n",
    "        merge(stats, left_on='var', right_on='var')\n",
    "    # include percentiles\n",
    "    stats = az.summary(cmdstan_data, var_names=varnames, stat_funcs=func_dict, extend=False, round_to=rounding).reset_index().rename(columns={'index': 'var'}).merge(stats, left_on='var', right_on='var')\n",
    "    stats['time'] = stats['var'].apply(lambda st: st[st.find(\"[\")+1:st.find(\"]\")])\n",
    "    stats['time'] = ['NA' if \"[\" not in y else x for x,y in zip(stats['time'],stats['var'])]\n",
    "    stats['var'] = stats['var'].apply(lambda st: st[:st.find(\"[\")] if \"[\" in st else st)\n",
    "    return stats.loc[:,['var','time','mean','hdi2.5','hdi25','hdi75','hdi97.5','q2.5','q25',\n",
    "                        'median','q75','q97.5','ess_bulk','ess_tail','r_hat']]\n",
    "\n",
    "clrs_ = [\"#00a1d5\", \"#fee391\", \"#d8daeb\", \"#bababa\", \"k\"] # via https://nanx.me/ggsci/index.html #blue = #74add1 #yellow = #fee391\n",
    "\n",
    "num_warmup = 1000\n",
    "num_iterations = 1250\n",
    "num_chains = 4\n",
    "\n",
    "import rpy2.rinterface\n",
    "%load_ext rpy2.ipython\n",
    "rpy2.robjects.r['options'](warn=-1)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='arviz')\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module='arviz')\n",
    "from pytz_deprecation_shim import PytzUsageWarning\n",
    "warnings.filterwarnings('ignore', category=PytzUsageWarning)\n",
    "\n",
    "import pyreadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e70ac1-f2e7-41a7-9ae9-4dda4dbb5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_userID = 'XXX'\n",
    "remote_userPSW = 'XXX'\n",
    "remote_IP = 'XXX'\n",
    "remote_ip = remote_IP\n",
    "\n",
    "num_warmup = 1000\n",
    "num_iterations = 1250\n",
    "num_chains = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e32f69-eb1f-445e-bbfe-04dcbe746ac0",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e865e-8962-4afc-8f20-d455b7cd1bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_upper = pd.to_datetime(\"2023-07-05\", format=\"%Y-%m-%d\")\n",
    "date_lower = date_upper - pd.DateOffset(days = 45)\n",
    "date_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158de19b-4da4-4f85-af8f-89103dd7b7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "mindate = pd.to_datetime(\"2023-05-15\", format=\"%Y-%m-%d\")\n",
    "mindate_to_show = pd.to_datetime(\"2023-05-01\", format=\"%Y-%m-%d\")\n",
    "maxdate_to_show = pd.to_datetime(\"2023-07-20\", format=\"%Y-%m-%d\")\n",
    "cutoff_date = pd.to_datetime(\"2023-07-15\", format=\"%Y-%m-%d\")\n",
    "truncation_date = pd.to_datetime(\"2023-08-15\", format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607feda9-a432-498c-bbe3-3dba78c21950",
   "metadata": {},
   "source": [
    "## A. Main epicurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a0aede-8e9d-4e8f-946a-45ec3b46c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases = pd.read_csv('../../data/epicurve_figure_1.csv')\n",
    "df_cases['Date'] = pd.to_datetime(df_cases['Date'], format=\"%Y-%m-%d\")\n",
    "df_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba284a1c-3c0f-4c7a-85a1-cc3f3e650ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clrs_ = [\"#00a1d5\", \"#fee391\", \"#d8daeb\", \"#bababa\", \"k\"]\n",
    "colors = ['#4575b4', '#fdae61']\n",
    "ymx = 23\n",
    "\n",
    "base = alt.Chart(df_cases).encode(\n",
    "    alt.X('Date', axis=alt.Axis(title=\"Date of symptom onset (month/day in 2023)\", format = (\"%m/%d\")), \n",
    "          scale=alt.Scale(domain=[mindate, maxdate_to_show]))\n",
    ")\n",
    "\n",
    "bar_cases_obs = base.mark_area(interpolate='step-before', color=clrs_[0], binSpacing=0, width=5.25, clip=True).encode(\n",
    "    alt.Y('Onset:Q', scale=alt.Scale(domain=[0, ymx]), sort=['local', 'imported'][::-1])\n",
    ").configure_range(\n",
    "        category=alt.RangeScheme(clrs_)\n",
    ").resolve_scale(y = 'independent').properties(width=400, height=300).configure_axis(grid=False)\n",
    "\n",
    "bar_cases_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de77f2-642b-4b0a-a571-48862a94d50b",
   "metadata": {},
   "source": [
    "## B. Loading linelist data from Hong Kong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1101749a-6181-44df-8c20-2154102025b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linelist_hk = pd.read_excel(os.path.join(\"../../data\", \"20230823-HongKong-linelist.xlsx\")).drop(['Source', 'Remarks'], axis=1)\n",
    "df_linelist_hk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b8e0f-0c20-4358-9c76-4f9cc3c5e29b",
   "metadata": {},
   "source": [
    "## C. Identifying some variable for the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e8d97-c53a-431f-b7a7-8693541bd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "TJul1_ = (pd.to_datetime('2023-07-01')-mindate).days + 1\n",
    "TAug1_ = (pd.to_datetime('2023-08-01')-mindate).days + 1\n",
    "TSep1_ = (pd.to_datetime('2023-09-01')-mindate).days + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e00f4-fd90-469e-a7ae-51804ccefff8",
   "metadata": {},
   "source": [
    "# <font color=\"green\">2. Analysis</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ae1bc-53b7-4dd7-8f89-b52023c6bb57",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">2a. Identifying the reporting delay from HK data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280046d1-c9b7-4ab6-9d3f-ddc04852cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncation_date_hk = pd.to_datetime('2023-08-23')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea5fe32-3eb3-4017-8f99-a91e75666dd6",
   "metadata": {},
   "source": [
    "### By reporting date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d8fe2e-c0d1-4a82-8b33-8db13acab561",
   "metadata": {},
   "outputs": [],
   "source": [
    "basename = truncation_date_hk.strftime(\"%Y%m%d\")+f'_reporting_delay_HK_gengamma'\n",
    "standirname = os.path.join(mainstandirname, basename)\n",
    "\n",
    "paths = [str(x) for x in list(pathlib.Path(standirname).glob(\"trace*.csv\"))]\n",
    "print(paths)\n",
    "fit_rep_delay = cmdstan.from_csv(paths)\n",
    "\n",
    "idata_rep_delay = az.from_cmdstanpy(posterior=fit_rep_delay)\n",
    "\n",
    "mod_delay_summary_output = get_stats(idata_rep_delay.posterior, ['mean_delay', 'sd_delay', 'a', 'sigma', 'mu', 'q', 'loga', 'logsigma'])\n",
    "mod_delay_summary_output['var'] = mod_delay_summary_output['var'].astype('string')\n",
    "for col in mod_delay_summary_output.columns[2:]:\n",
    "    mod_delay_summary_output[col] = mod_delay_summary_output[col].astype('float')\n",
    "mod_delay_summary_output['Mean (95% CI)'] = [\"%.2f (%.2f, %.2f)\" % (x, y, z) for (x,y,z) in zip(mod_delay_summary_output['mean'], mod_delay_summary_output['q2.5'], mod_delay_summary_output['q97.5'])]\n",
    "mod_delay_summary_output['Parameter'] = ['Mean delay, days','SD, days','a', 'sigma', 'mu', 'q', 'loga', 'logsigma'] \n",
    "\n",
    "az.plot_trace(idata_rep_delay, var_names=('mean_delay', 'sd_delay')); plt.tight_layout()\n",
    "mod_delay_summary = az.summary(idata_rep_delay, var_names=['mean_delay', 'sd_delay'], hdi_prob=0.95)\n",
    "\n",
    "display(mod_delay_summary_output.loc[:, ['Parameter', 'Mean (95% CI)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92dcfac-727a-4eb0-9bbc-155b123241b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_summary_rep_delay = az.summary(idata_rep_delay, ['mu', 'loga', 'logsigma'])\n",
    "stats_summary_rep_delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d3dd9-f185-4802-b0b6-0aafcef9d722",
   "metadata": {},
   "source": [
    "### By diagnosis date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66845db2-20ee-418d-a6a8-67a993e2f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "basename = truncation_date_hk.strftime(\"%Y%m%d\")+f'_reporting_delay_HK_gengamma-diangosis'\n",
    "standirname = os.path.join(mainstandirname, basename)\n",
    "\n",
    "paths = [str(x) for x in list(pathlib.Path(standirname).glob(\"trace*.csv\"))]\n",
    "print(paths)\n",
    "fit_rep_delay_diagnosis = cmdstan.from_csv(paths)\n",
    "\n",
    "idata_rep_delay_diagnosis = az.from_cmdstanpy(posterior=fit_rep_delay_diagnosis)\n",
    "\n",
    "mod_delay_diagnosis_summary_output = get_stats(idata_rep_delay_diagnosis.posterior, ['mean_delay', 'sd_delay', 'a', 'sigma', 'mu', 'q', 'loga', 'logsigma'])\n",
    "mod_delay_diagnosis_summary_output['var'] = mod_delay_diagnosis_summary_output['var'].astype('string')\n",
    "for col in mod_delay_diagnosis_summary_output.columns[2:]:\n",
    "    mod_delay_diagnosis_summary_output[col] = mod_delay_diagnosis_summary_output[col].astype('float')\n",
    "mod_delay_diagnosis_summary_output['Mean (95% CI)'] = [\"%.2f (%.2f, %.2f)\" % (x, y, z) for (x,y,z) in zip(mod_delay_diagnosis_summary_output['mean'], mod_delay_diagnosis_summary_output['q2.5'], mod_delay_diagnosis_summary_output['q97.5'])]\n",
    "mod_delay_diagnosis_summary_output['Parameter'] = ['Mean delay, days','SD, days','a', 'sigma', 'mu', 'q', 'loga', 'logsigma'] \n",
    "\n",
    "az.plot_trace(idata_rep_delay_diagnosis, var_names=('mean_delay', 'sd_delay')); plt.tight_layout()\n",
    "mod_delay_diagnosis_summary = az.summary(idata_rep_delay_diagnosis, var_names=['mean_delay', 'sd_delay'], hdi_prob=0.95)\n",
    "\n",
    "display(mod_delay_diagnosis_summary_output.loc[:, ['Parameter', 'Mean (95% CI)']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc410f-9781-448f-9943-07cb5c6eaa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_summary_rep_delay_diagnosis = az.summary(idata_rep_delay_diagnosis, ['mu', 'loga', 'logsigma'])\n",
    "stats_summary_rep_delay_diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e73f11e-dbe2-45fa-aeaf-3681e55bc781",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <font color=\"orange\">2b. Estimating the R0 in Mainland China</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8762ff43-9b51-46ad-9c62-f001c1604e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_code_exp_growth = \"\"\"functions {\n",
    "    real gengamma_cdf(real x, real q, real mu, real sigma) {\n",
    "        real logx = log(x),\n",
    "            z = (logx - mu) / sigma,\n",
    "            a = inv_square(q),\n",
    "            value = gamma_cdf(a * exp(q * z) | a, 1);\n",
    "\n",
    "        return value;\n",
    "    }\n",
    "\n",
    "    /* discretized version */\n",
    "    vector dgengamma(real q, real mu, real sigma, int D) {\n",
    "        vector[D] res;\n",
    "        for (k in 1:D)\n",
    "            res[k] = gengamma_cdf(k - 0.5 | q, mu, sigma);\n",
    "\n",
    "        if (D > 1)\n",
    "            return append_row(append_row(res[1], tail(res, D-1) - head(res, D-1)), 1.0 - res[D]);\n",
    "        else \n",
    "            return to_vector({res[1], 1 - res[1]});\n",
    "    }\n",
    "\n",
    "    vector dgamma(real param1, real param2, int K) {\n",
    "        vector[K] res;\n",
    "        for (k in 1:K)\n",
    "            res[k] = gamma_cdf(k - 0.5 | param1, param2);\n",
    "\n",
    "        return append_row(res[1], tail(res, K-1) - head(res, K-1));\n",
    "    }\n",
    "}\n",
    "\n",
    "data {\n",
    "    int<lower = 1> Tlower, Tupper; // the cutoffday for the estimation of the exponential growth \n",
    "    array[Tupper] int<lower = 0> cases_onset; // number of cases by date of symptom onset starting to be recorded till the day Tupper\n",
    "\n",
    "    int<lower = Tupper> T; // total number of days for which we have the data on number of cases by date of notification (their symptom onset date is missed)\n",
    "    array[T] int<lower = 0> cases_reported, cases_diagnosis;\n",
    "\n",
    "    int<lower = Tupper> Tpred;\n",
    "    int<upper = Tpred> TJul1, TAug1, TSep1;\n",
    "\n",
    "    // reporting delay described by the generalized gamma distribution\n",
    "    real loga, logsigma, mu;\n",
    "\n",
    "    // delay b/w onset and diagnosis described by the generalized gamma distribution\n",
    "    real loga_diagnosis, logsigma_diagnosis, mu_diagnosis;\n",
    "\n",
    "    // generation time (scale and shape of the Gamma distribution estimated in Guzetta et al. 2022)\n",
    "    real<lower = 0> genalpha, geninvbeta;\n",
    "}\n",
    "\n",
    "transformed data {\n",
    "    // reporting delay\n",
    "    real a = exp(loga), q = inv_sqrt(a), sigma = exp(logsigma);\n",
    "\n",
    "    // delay b/w onset and diagnosis\n",
    "    real a_diagnosis = exp(loga_diagnosis), q_diagnosis = inv_sqrt(a_diagnosis), sigma_diagnosis = exp(logsigma_diagnosis);\n",
    "\n",
    "    // generation time\n",
    "    real genmean = genalpha * geninvbeta,\n",
    "        gensigma = sqrt(genalpha) * geninvbeta;\n",
    "\n",
    "    // backprojecting from the reporting date\n",
    "    array[T] int cases_onset_backprj = rep_array(0, T);\n",
    "    for (t in 1:T) \n",
    "        if (cases_reported[t] > 0) {\n",
    "            vector[t+1] probs_for_backprojection = dgengamma(q, mu, sigma, t);\n",
    "            array[t+1] int counts_backprj = multinomial_rng(probs_for_backprojection, cases_reported[t]);\n",
    "            for (i in 1:t) \n",
    "                cases_onset_backprj[t-i+1] += counts_backprj[i];\n",
    "        }\n",
    "\n",
    "    // backprojecting from the diagnosis date\n",
    "    for (t in 1:T) \n",
    "        if (cases_diagnosis[t] > 0) {\n",
    "            vector[t+1] probs_for_backprojection = dgengamma(q_diagnosis, mu_diagnosis, sigma_diagnosis, t);\n",
    "            array[t+1] int counts_backprj = multinomial_rng(probs_for_backprojection, cases_diagnosis[t]);\n",
    "            for (i in 1:t) \n",
    "                cases_onset_backprj[t-i+1] += counts_backprj[i];\n",
    "        }\n",
    "\n",
    "\n",
    "    array[Tupper] int cases;\n",
    "    for (t in 1:Tupper) \n",
    "        cases[t] = cases_onset[t] + cases_onset_backprj[t];\n",
    "\n",
    "    real jitter = 1e-9;\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    // exponential growth rate\n",
    "    real logr;\n",
    "    // initial incidence\n",
    "    real<lower = 0> i0;\n",
    "    // process error\n",
    "    real<lower = 0> phi;\n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "    real r = exp(logr);\n",
    "}\n",
    "\n",
    "model {\n",
    "    logr ~ std_normal();\n",
    "    i0 ~ normal(5, 10);\n",
    "    phi ~ gamma(1, 1);\n",
    "\n",
    "    for (t in Tlower:Tupper)\n",
    "        if (cases[t] > 0)\n",
    "            target += neg_binomial_2_lupmf(cases[t] | i0 * exp(r * t) + jitter, phi); \n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    // basic reproduction number\n",
    "    real R0_norm_approx = exp(r * genmean - 0.5 * square(r) * square(gensigma)),\n",
    "        R0 = pow(1 + r * geninvbeta, genalpha);\n",
    "\n",
    "    // doubling time\n",
    "    real doubling_time = log(2) / r; \n",
    "\n",
    "    array[Tpred] int cases_prj = rep_array(0, Tpred);\n",
    "    for (t in 1:Tpred)\n",
    "        cases_prj[t] = (t > Tupper) ? neg_binomial_2_rng(i0 * exp(r * t), phi) : cases[t];\n",
    "\n",
    "    int casesJul = sum(cases_prj[TJul1:TAug1-1]), \n",
    "        casesAug = sum(cases_prj[TAug1:TSep1-1]);\n",
    "\n",
    "    array[Tpred] int cases_reported_prj = rep_array(0, Tpred);\n",
    "    for (t in 1:Tpred) \n",
    "        if (cases_prj[t] > 0) {\n",
    "            vector[Tpred-t+2] probs_for_prj = dgengamma(q, mu, sigma, Tpred-t+1);\n",
    "            array[Tpred-t+2] int counts = multinomial_rng(probs_for_prj, cases_prj[t]);\n",
    "            for (s in 1:Tpred-t+1)\n",
    "                cases_reported_prj[t+s-1] += counts[s];\n",
    "        }\n",
    "\n",
    "    int reportedJul = sum(cases_reported_prj[TJul1:TAug1-1]), \n",
    "        reportedAug = sum(cases_reported_prj[TAug1:TSep1-1]);\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895bd3c1-05f6-4bba-aa97-78d68f1cee96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sim(Tlower_, Tupper_):\n",
    "    basename = truncation_date_hk.strftime(\"%Y%m%d\")+f'_exp_growth_China_expwindow_{Tlower_}-{Tupper_}_Miura2023'\n",
    "    standirname = os.path.join(mainstandirname, basename)\n",
    "    !rm {standirname}/*\n",
    "    os.makedirs(standirname, exist_ok=True)\n",
    "    stanscriptdir = '../Dropbox/'+standirname[9:]\n",
    "\n",
    "    # Miura et al. 2023\n",
    "    mean_gt_ = 10.1; sd_gt_ = 6.1\n",
    "    genalpha_ = (mean_gt_ / sd_gt_)**2\n",
    "    geninvbeta_ = (sd_gt_**2) / mean_gt_ \n",
    "\n",
    "    Df_ = df_cases\n",
    "    \n",
    "    stan_data = dict({\n",
    "        'Tlower': Tlower_,\n",
    "        'Tupper': Tupper_,\n",
    "        'Tpred': TSep1_+1,\n",
    "        'TJul1': TJul1_,\n",
    "        'TAug1': TAug1_,\n",
    "        'TSep1': TSep1_,\n",
    "        'cases_onset': Df_['Onset'].astype('int64').values[:Tupper_],\n",
    "        'T': Df_.shape[0],\n",
    "        'cases_reported': Df_['Reported'].astype('int64').values,\n",
    "        'loga': stats_summary_rep_delay.loc['loga']['mean'],\n",
    "        'logsigma': stats_summary_rep_delay.loc['logsigma']['mean'],\n",
    "        'mu': stats_summary_rep_delay.loc['mu']['mean'],\n",
    "        'cases_diagnosis': Df_['Diagnosis'].astype('int64').values,\n",
    "        'loga_diagnosis': stats_summary_rep_delay_diagnosis.loc['loga']['mean'],\n",
    "        'logsigma_diagnosis': stats_summary_rep_delay_diagnosis.loc['logsigma']['mean'],\n",
    "        'mu_diagnosis': stats_summary_rep_delay_diagnosis.loc['mu']['mean'],\n",
    "        'genalpha': genalpha_,\n",
    "        'geninvbeta': geninvbeta_\n",
    "    })\n",
    "    stan_data_file = os.path.join(standirname, 'Data.json')\n",
    "    cmdstan.write_stan_json(stan_data_file, stan_data)\n",
    "\n",
    "    stan_inits = dict({\n",
    "        'r': 0.1,\n",
    "        'phi': 1.0\n",
    "    })\n",
    "    stan_init_file = os.path.join(standirname, 'Inits.json')\n",
    "    cmdstan.write_stan_json(stan_init_file, stan_inits)\n",
    "\n",
    "    stan_code_file = os.path.join(standirname, f'fit_exp_growth.stan')\n",
    "    with open(stan_code_file, \"w+\") as f:\n",
    "        f.write(stan_code_exp_growth)\n",
    "        f.close()\n",
    "    \n",
    "    model = cmdstan.CmdStanModel(stan_file=stan_code_file, cpp_options={'STAN_THREADS': 'TRUE'}, compile='force')\n",
    "    fit = model.sample(data=stan_data_file, seed = 1, iter_warmup=num_warmup, iter_sampling=1, inits=stan_init_file, parallel_chains=10,\n",
    "                       show_console=False, show_progress=False, chains = 4000)\n",
    "    fit.save_csvfiles(dir=standirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392c8c40-3a54-4072-8abe-22bf546046b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for date_lower_ in pd.date_range(pd.to_datetime('2023-05-16'), pd.to_datetime('2023-05-26')):\n",
    "    for date_upper_ in pd.date_range(pd.to_datetime('2023-06-30'), pd.to_datetime('2023-07-10')):\n",
    "        Tlower_ = (date_lower_-mindate).days + 1\n",
    "        Tupper_ = (date_upper_-mindate).days + 1\n",
    "        print([date_lower_, date_upper_], [Tlower_, Tupper_])\n",
    "        sim(Tlower_, Tupper_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349cd52f-fd1d-4355-900b-8129f29574df",
   "metadata": {},
   "outputs": [],
   "source": [
    "basename = truncation_date_hk.strftime(\"%Y%m%d\")+f'_exp_growth_China_expwindow_{Tlower_}-{Tupper_}_Miura2023'\n",
    "standirname = os.path.join(mainstandirname, basename)\n",
    "idata_chn = az.from_cmdstanpy(cmdstan.from_csv(path=standirname))\n",
    "df_stats_chn = get_stats(idata_chn.posterior, ['r', 'i0', 'phi', 'R0', 'doubling_time']) \n",
    "df_stats_chn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe88a46-63e6-4dc3-8265-9c074dcb1529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Df_stats = None\n",
    "fldrs = !ls {mainstandirname} | grep _expwindow_ | grep Miura2023\n",
    "for fldr in fldrs:\n",
    "    print(fldr)\n",
    "    standirname = os.path.join(mainstandirname, fldr)\n",
    "    Tlower_ = int(fldr.split(\"_expwindow_\")[-1].split(\"-\")[0])\n",
    "    Tupper_ = int(fldr.split(\"_expwindow_\")[-1].split(\"-\")[-1].split(\"_\")[0])\n",
    "    idata_chn = az.from_cmdstanpy(cmdstan.from_csv(path=standirname))\n",
    "    df_stats_ = get_stats(idata_chn.posterior, ['R0', 'doubling_time', 'reportedJul', 'reportedAug']) \n",
    "    df_stats_['Tlower'] = Tlower_\n",
    "    df_stats_['Tupper'] = Tupper_\n",
    "    df_stats_ = df_stats_.drop(['ess_bulk', 'ess_tail', 'r_hat'], axis=1)\n",
    "    Df_stats = df_stats_ if Df_stats is None else pd.concat([Df_stats, df_stats_], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e31ce-1d84-4599-afe1-fbc4e433bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f97ebe7-4071-4c00-b709-cd8cbcd019bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_stats.Tlower.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25b325c-8f11-482e-aca0-ac0b7366264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_stats['date_lower'] = [mindate + pd.DateOffset(days = x - 1) for x in Df_stats['Tlower']]\n",
    "Df_stats['date_upper'] = [mindate + pd.DateOffset(days = x - 1) for x in Df_stats['Tupper']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ef9c0-edba-446a-9387-2f6af2c9bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_stats_table = Df_stats.loc[lambda d: d['var']=='R0', ['Tlower', 'Tupper', 'median']].pivot_table(values='median', index='Tlower', columns='Tupper')\n",
    "Df_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df1b29f-04a2-46f1-bf0f-1e8776e7ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_stats.date_lower.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbbac66-b534-4364-8314-36f6a4c0c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_stats.date_upper.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a795cb8-5dc0-425a-9c46-be5576a8608b",
   "metadata": {},
   "source": [
    "### Estimates of Re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae7552-3676-4d1d-b0c7-3aa19c8ce79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_ = 300; ht_ = 270\n",
    "heatmap = alt.Chart(Df_stats.loc[lambda d: d['var']=='R0', ['date_lower', 'date_upper', 'median']]).mark_rect().encode(\n",
    "    x=alt.X('yearmonthdate(date_lower)', title=\"Lower bound date\", axis=alt.Axis(format='%m/%d', labelAngle=-90)),\n",
    "    y=alt.Y('yearmonthdate(date_upper)', title=\"Upper bound date\", axis=alt.Axis(format='%m/%d', labelAngle=0), scale=alt.Scale(reverse=True)),\n",
    "    color=alt.Color('median:Q', scale=alt.Scale(scheme='greens', reverse=False), title=\"median\")\n",
    ")\n",
    "\n",
    "Df_stats_ = Df_stats.loc[lambda d: d['var']=='R0', ['date_lower', 'date_upper', 'median']]\n",
    "print(\"Min median:\", Df_stats_['median'].min())\n",
    "print(\"Max median:\", Df_stats_['median'].max())\n",
    "\n",
    "chart = alt.layer(heatmap).properties(width=wd_, height=ht_)\n",
    "save_chart(chart, f'../../figures/sensitivity-Re-median.svg')\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f126b2-007d-44f9-9fc0-f8c40977eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = alt.Chart(Df_stats.loc[lambda d: d['var']=='R0', ['date_lower', 'date_upper', 'q2.5']].rename(columns={'q2.5':'lower'})).mark_rect().encode(\n",
    "    x=alt.X('yearmonthdate(date_lower)', title=\"Lower bound date\", axis=alt.Axis(format='%m/%d', labelAngle=-90)),\n",
    "    y=alt.Y('yearmonthdate(date_upper)', title=\"Upper bound date\", axis=alt.Axis(format='%m/%d', labelAngle=0), scale=alt.Scale(reverse=True)),\n",
    "    color=alt.Color('lower:Q', scale=alt.Scale(scheme='greens', reverse=False), title=\"lower\") #lighttealblue\n",
    ")\n",
    "\n",
    "Df_stats_ = Df_stats.loc[lambda d: d['var']=='R0', ['date_lower', 'date_upper', 'q2.5']]\n",
    "print(\"Min lower:\", Df_stats_['q2.5'].min())\n",
    "print(\"Max lower:\", Df_stats_['q2.5'].max())\n",
    "\n",
    "chart = alt.layer(heatmap).properties(width=wd_, height=ht_)\n",
    "save_chart(chart, f'../../figures/sensitivity-Re-lower.svg')\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72d72f-61fa-4d2e-810d-1e25015792b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = alt.Chart(Df_stats.loc[lambda d: d['var']=='R0', ['date_lower', 'date_upper', 'q97.5']].rename(columns={'q97.5':'upper'})).mark_rect().encode(\n",
    "    x=alt.X('yearmonthdate(date_lower)', title=\"Lower bound date\", axis=alt.Axis(format='%m/%d', labelAngle=-90)),\n",
    "    y=alt.Y('yearmonthdate(date_upper)', title=\"Upper bound date\", axis=alt.Axis(format='%m/%d', labelAngle=0), scale=alt.Scale(reverse=True)),\n",
    "    color=alt.Color('upper:Q', scale=alt.Scale(scheme='greens', reverse=False), title=\"upper\") #tealblues\n",
    ")\n",
    "\n",
    "Df_stats_ = Df_stats.loc[lambda d: d['var']=='R0', ['date_lower', 'date_upper', 'q97.5']]\n",
    "print(\"Min upper:\", Df_stats_['q97.5'].min())\n",
    "print(\"Max upper:\", Df_stats_['q97.5'].max())\n",
    "\n",
    "chart = alt.layer(heatmap).properties(width=wd_, height=ht_)\n",
    "save_chart(chart, f'../../figures/sensitivity-Re-upper.svg')\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9db1f85-f818-4f7f-96b9-8f18dae67a97",
   "metadata": {},
   "source": [
    "### Reported cases in July"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c9c11c-5916-4bc4-ad3f-a40e9e944353",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = alt.Chart(Df_stats.loc[lambda d: d['var']=='reportedJul', ['date_lower', 'date_upper', 'median']]).mark_rect().encode(\n",
    "    x=alt.X('yearmonthdate(date_lower)', title=\"Lower bound date\", axis=alt.Axis(format='%m/%d', labelAngle=-90)),\n",
    "    y=alt.Y('yearmonthdate(date_upper)', title=\"Upper bound date\", axis=alt.Axis(format='%m/%d', labelAngle=0), scale=alt.Scale(reverse=True)),\n",
    "    color=alt.Color('median:Q', scale=alt.Scale(scheme='lighttealblue', reverse=False), title=\"median\")\n",
    ")\n",
    "\n",
    "Df_stats_ = Df_stats.loc[lambda d: d['var']=='reportedJul', ['date_lower', 'date_upper', 'median']]\n",
    "print(\"Min median:\", Df_stats_['median'].min())\n",
    "print(\"Max median:\", Df_stats_['median'].max())\n",
    "\n",
    "chart = alt.layer(heatmap).properties(width=wd_, height=ht_)\n",
    "save_chart(chart, f'../../figures/sensitivity-reportedJul-median.svg')\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2cb49-bef3-468b-b9dd-6d1d80f2a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = alt.Chart(Df_stats.loc[lambda d: d['var']=='reportedJul', ['date_lower', 'date_upper', 'q2.5']].rename(columns={'q2.5':'lower'})).mark_rect().encode(\n",
    "    x=alt.X('yearmonthdate(date_lower)', title=\"Lower bound date\", axis=alt.Axis(format='%m/%d', labelAngle=-90)),\n",
    "    y=alt.Y('yearmonthdate(date_upper)', title=\"Upper bound date\", axis=alt.Axis(format='%m/%d', labelAngle=0), scale=alt.Scale(reverse=True)),\n",
    "    color=alt.Color('lower:Q', scale=alt.Scale(scheme='lighttealblue', reverse=False), title=\"lower\") #lighttealblue\n",
    ")\n",
    "\n",
    "Df_stats_ = Df_stats.loc[lambda d: d['var']=='reportedJul', ['date_lower', 'date_upper', 'q2.5']]\n",
    "print(\"Min lower:\", Df_stats_['q2.5'].min())\n",
    "print(\"Max lower:\", Df_stats_['q2.5'].max())\n",
    "\n",
    "chart = alt.layer(heatmap).properties(width=wd_, height=ht_)\n",
    "save_chart(chart, f'../../figures/sensitivity-reportedJul-lower.svg')\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab078d8-d9f8-490e-837a-576ca716f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = alt.Chart(Df_stats.loc[lambda d: d['var']=='reportedJul', ['date_lower', 'date_upper', 'q97.5']].rename(columns={'q97.5':'upper'})).mark_rect().encode(\n",
    "    x=alt.X('yearmonthdate(date_lower)', title=\"Lower bound date\", axis=alt.Axis(format='%m/%d', labelAngle=-90)),\n",
    "    y=alt.Y('yearmonthdate(date_upper)', title=\"Upper bound date\", axis=alt.Axis(format='%m/%d', labelAngle=0), scale=alt.Scale(reverse=True)),\n",
    "    color=alt.Color('upper:Q', scale=alt.Scale(scheme='lighttealblue', reverse=False), title=\"upper\") #tealblues\n",
    ")\n",
    "\n",
    "Df_stats_ = Df_stats.loc[lambda d: d['var']=='reportedJul', ['date_lower', 'date_upper', 'q97.5']]\n",
    "print(\"Min upper:\", Df_stats_['q97.5'].min())\n",
    "print(\"Max upper:\", Df_stats_['q97.5'].max())\n",
    "\n",
    "chart = alt.layer(heatmap).properties(width=wd_, height=ht_)\n",
    "save_chart(chart, f'../../figures/sensitivity-reportedJul-upper.svg')\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d982899e-789b-4693-945f-c9ea4bdd9ec1",
   "metadata": {},
   "source": [
    "### Reported cases in August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba829ca-82fc-4385-b4a4-cd15acd71954",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = alt.Chart(Df_stats.loc[lambda d: d['var']=='reportedAug', ['date_lower', 'date_upper', 'median']]).mark_rect().encode(\n",
    "    x=alt.X('yearmonthdate(date_lower)', title=\"Lower bound date\", axis=alt.Axis(format='%m/%d', labelAngle=-90)),\n",
    "    y=alt.Y('yearmonthdate(date_upper)', title=\"Upper bound date\", axis=alt.Axis(format='%m/%d', labelAngle=0), scale=alt.Scale(reverse=True)),\n",
    "    color=alt.Color('median:Q', scale=alt.Scale(scheme='blues', reverse=False), title=\"median\")\n",
    ")\n",
    "\n",
    "Df_stats_ = Df_stats.loc[lambda d: d['var']=='reportedAug', ['date_lower', 'date_upper', 'median']]\n",
    "print(\"Min median:\", Df_stats_['median'].min())\n",
    "print(\"Max median:\", Df_stats_['median'].max())\n",
    "\n",
    "chart = alt.layer(heatmap).properties(width=wd_, height=ht_)\n",
    "save_chart(chart, f'../../figures/sensitivity-reportedAug-median.svg')\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b7652-1246-45d3-8a7d-d832c7b77842",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = alt.Chart(Df_stats.loc[lambda d: d['var']=='reportedAug', ['date_lower', 'date_upper', 'q2.5']].rename(columns={'q2.5':'lower'})).mark_rect().encode(\n",
    "    x=alt.X('yearmonthdate(date_lower)', title=\"Lower bound date\", axis=alt.Axis(format='%m/%d', labelAngle=-90)),\n",
    "    y=alt.Y('yearmonthdate(date_upper)', title=\"Upper bound date\", axis=alt.Axis(format='%m/%d', labelAngle=0), scale=alt.Scale(reverse=True)),\n",
    "    color=alt.Color('lower:Q', scale=alt.Scale(scheme='blues', reverse=False), title=\"lower\")\n",
    ")\n",
    "\n",
    "Df_stats_ = Df_stats.loc[lambda d: d['var']=='reportedAug', ['date_lower', 'date_upper', 'q2.5']]\n",
    "print(\"Min lower:\", Df_stats_['q2.5'].min())\n",
    "print(\"Max lower:\", Df_stats_['q2.5'].max())\n",
    "\n",
    "chart = alt.layer(heatmap).properties(width=wd_, height=ht_)\n",
    "save_chart(chart, f'../../figures/sensitivity-reportedAug-lower.svg')\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4072c038-580e-4e0c-b697-0dadb2907b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = alt.Chart(Df_stats.loc[lambda d: d['var']=='reportedAug', ['date_lower', 'date_upper', 'q97.5']].rename(columns={'q97.5':'upper'})).mark_rect().encode(\n",
    "    x=alt.X('yearmonthdate(date_lower)', title=\"Lower bound date\", axis=alt.Axis(format='%m/%d', labelAngle=-90)),\n",
    "    y=alt.Y('yearmonthdate(date_upper)', title=\"Upper bound date\", axis=alt.Axis(format='%m/%d', labelAngle=0), scale=alt.Scale(reverse=True)),\n",
    "    color=alt.Color('upper:Q', scale=alt.Scale(scheme='blues', reverse=False), title=\"upper\") #tealblues\n",
    ")\n",
    "\n",
    "Df_stats_ = Df_stats.loc[lambda d: d['var']=='reportedAug', ['date_lower', 'date_upper', 'q97.5']]\n",
    "print(\"Min upper:\", Df_stats_['q97.5'].min())\n",
    "print(\"Max upper:\", Df_stats_['q97.5'].max())\n",
    "\n",
    "chart = alt.layer(heatmap).properties(width=wd_, height=ht_)\n",
    "save_chart(chart, f'../../figures/sensitivity-reportedAug-upper.svg')\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bfc373-3e8b-427d-af68-ced08a2b23eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <font color=\"orange\">2c. Considering only symptomatic cases</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23541108-f389-4ef5-ae61-a5bb0c502078",
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_code_exp_growth_only_symptomatic = \"\"\"functions {\n",
    "    real gengamma_cdf(real x, real q, real mu, real sigma) {\n",
    "        real logx = log(x),\n",
    "            z = (logx - mu) / sigma,\n",
    "            a = inv_square(q),\n",
    "            value = gamma_cdf(a * exp(q * z) | a, 1);\n",
    "\n",
    "        return value;\n",
    "    }\n",
    "\n",
    "    /* discretized version */\n",
    "    vector dgengamma(real q, real mu, real sigma, int D) {\n",
    "        vector[D] res;\n",
    "        for (k in 1:D)\n",
    "            res[k] = gengamma_cdf(k - 0.5 | q, mu, sigma);\n",
    "\n",
    "        if (D > 1)\n",
    "            return append_row(append_row(res[1], tail(res, D-1) - head(res, D-1)), 1.0 - res[D]);\n",
    "        else \n",
    "            return to_vector({res[1], 1 - res[1]});\n",
    "    }\n",
    "\n",
    "    vector dgamma(real param1, real param2, int K) {\n",
    "        vector[K] res;\n",
    "        for (k in 1:K)\n",
    "            res[k] = gamma_cdf(k - 0.5 | param1, param2);\n",
    "\n",
    "        return append_row(res[1], tail(res, K-1) - head(res, K-1));\n",
    "    }\n",
    "}\n",
    "\n",
    "data {\n",
    "    int<lower = 1> Tlower, Tupper; // the cutoffday for the estimation of the exponential growth \n",
    "    array[Tupper] int<lower = 0> cases_onset; // number of cases by date of symptom onset starting to be recorded till the day Tupper\n",
    "\n",
    "    int<lower = Tupper> Tpred;\n",
    "    int<upper = Tpred> TJul1, TAug1, TSep1;\n",
    "\n",
    "    // reporting delay described by the generalized gamma distribution\n",
    "    real loga, logsigma, mu;\n",
    "\n",
    "    // generation time (scale and shape of the Gamma distribution estimated in Guzetta et al. 2022)\n",
    "    real<lower = 0> genalpha, geninvbeta;\n",
    "}\n",
    "\n",
    "transformed data {\n",
    "    // reporting delay\n",
    "    real a = exp(loga), q = inv_sqrt(a), sigma = exp(logsigma);\n",
    "\n",
    "    // generation time\n",
    "    real genmean = genalpha * geninvbeta,\n",
    "        gensigma = sqrt(genalpha) * geninvbeta;\n",
    "          \n",
    "    real jitter = 1e-9;\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    // exponential growth rate\n",
    "    real logr;\n",
    "    // initial incidence\n",
    "    real<lower = 0> i0;\n",
    "    // process error\n",
    "    real<lower = 0> phi;\n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "    real r = exp(logr);\n",
    "}\n",
    "\n",
    "model {\n",
    "    logr ~ std_normal();\n",
    "    i0 ~ normal(5, 10);\n",
    "    phi ~ gamma(1, 1);\n",
    "\n",
    "    for (t in Tlower:Tupper)\n",
    "        if (cases_onset[t] > 0)\n",
    "            target += neg_binomial_2_lupmf(cases_onset[t] | i0 * exp(r * t) + jitter, phi); \n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    // basic reproduction number\n",
    "    real R0_norm_approx = exp(r * genmean - 0.5 * square(r) * square(gensigma)),\n",
    "        R0 = pow(1 + r * geninvbeta, genalpha);\n",
    "\n",
    "    // doubling time\n",
    "    real doubling_time = log(2) / r; \n",
    "\n",
    "    array[Tpred] int cases_prj = rep_array(0, Tpred);\n",
    "    for (t in 1:Tpred)\n",
    "        cases_prj[t] = (t > Tupper) ? neg_binomial_2_rng(i0 * exp(r * t), phi) : cases_onset[t];\n",
    "\n",
    "    int casesJul = sum(cases_prj[TJul1:TAug1-1]), \n",
    "        casesAug = sum(cases_prj[TAug1:TSep1-1]);\n",
    "\n",
    "    array[Tpred] int cases_reported_prj = rep_array(0, Tpred);\n",
    "    for (t in 1:Tpred) \n",
    "        if (cases_prj[t] > 0) {\n",
    "            vector[Tpred-t+2] probs_for_prj = dgengamma(q, mu, sigma, Tpred-t+1);\n",
    "            array[Tpred-t+2] int counts = multinomial_rng(probs_for_prj, cases_prj[t]);\n",
    "            for (s in 1:Tpred-t+1)\n",
    "                cases_reported_prj[t+s-1] += counts[s];\n",
    "        }\n",
    "\n",
    "    int reportedJul = sum(cases_reported_prj[TJul1:TAug1-1]), \n",
    "        reportedAug = sum(cases_reported_prj[TAug1:TSep1-1]);\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f84bfb5-3b69-449c-81a8-d7eba7d7ef03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sim_only_sympt(Tlower_, Tupper_):\n",
    "    basename = truncation_date_hk.strftime(\"%Y%m%d\")+f'_exp_growth_China_only_sympt_window_{Tlower_}-{Tupper_}_Miura2023'\n",
    "    standirname = os.path.join(mainstandirname, basename)\n",
    "    !rm {standirname}/*\n",
    "    os.makedirs(standirname, exist_ok=True)\n",
    "    stanscriptdir = '../Dropbox/'+standirname[9:]\n",
    "\n",
    "    # Miura et al. 2023\n",
    "    mean_gt_ = 10.1; sd_gt_ = 6.1\n",
    "    genalpha_ = (mean_gt_ / sd_gt_)**2\n",
    "    geninvbeta_ = (sd_gt_**2) / mean_gt_ \n",
    "\n",
    "    Df_ = df_cases\n",
    "    \n",
    "    stan_data = dict({\n",
    "        'Tlower': Tlower_,\n",
    "        'Tupper': Tupper_,\n",
    "        'Tpred': TSep1_+1,\n",
    "        'TJul1': TJul1_,\n",
    "        'TAug1': TAug1_,\n",
    "        'TSep1': TSep1_,\n",
    "        'cases_onset': Df_['Onset'].astype('int64').values[:Tupper_],\n",
    "        'T': Df_.shape[0],\n",
    "        'loga': stats_summary_rep_delay.loc['loga']['mean'],\n",
    "        'logsigma': stats_summary_rep_delay.loc['logsigma']['mean'],\n",
    "        'mu': stats_summary_rep_delay.loc['mu']['mean'],\n",
    "        'loga_diagnosis': stats_summary_rep_delay_diagnosis.loc['loga']['mean'],\n",
    "        'logsigma_diagnosis': stats_summary_rep_delay_diagnosis.loc['logsigma']['mean'],\n",
    "        'mu_diagnosis': stats_summary_rep_delay_diagnosis.loc['mu']['mean'],\n",
    "        'genalpha': genalpha_,\n",
    "        'geninvbeta': geninvbeta_\n",
    "    })\n",
    "    stan_data_file = os.path.join(standirname, 'Data.json')\n",
    "    cmdstan.write_stan_json(stan_data_file, stan_data)\n",
    "\n",
    "    stan_inits = dict({\n",
    "        'r': 0.1,\n",
    "        'phi': 1.0\n",
    "    })\n",
    "    stan_init_file = os.path.join(standirname, 'Inits.json')\n",
    "    cmdstan.write_stan_json(stan_init_file, stan_inits)\n",
    "\n",
    "    stan_code_file = os.path.join(standirname, f'fit_exp_growth.stan')\n",
    "    with open(stan_code_file, \"w+\") as f:\n",
    "        f.write(stan_code_exp_growth_only_symptomatic)\n",
    "        f.close()\n",
    "    \n",
    "    model = cmdstan.CmdStanModel(stan_file=stan_code_file)\n",
    "    fit = model.sample(data=stan_data_file, seed = 1, iter_warmup=num_warmup, iter_sampling=1000, inits=stan_init_file,\n",
    "                       show_console=False, show_progress=False, chains = 4)\n",
    "    fit.save_csvfiles(dir=standirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88536cc1-dffe-4e34-b912-f5b24330599e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date_upper_ = pd.to_datetime(\"2023-07-05\", format=\"%Y-%m-%d\")\n",
    "date_lower_ = date_upper_ - pd.DateOffset(days = 45)\n",
    "\n",
    "Tlower_ = (date_lower_-mindate).days + 1\n",
    "Tupper_ = (date_upper_-mindate).days + 1\n",
    "print([date_lower_, date_upper_], [Tlower_, Tupper_])\n",
    "sim_only_sympt(Tlower_, Tupper_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c788e56-395b-44df-af62-1c23ef69cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Df_stats_only_sympt = None\n",
    "fldrs = !ls {mainstandirname} | grep _window_ | grep Miura2023\n",
    "for fldr in fldrs:\n",
    "    print(fldr)\n",
    "    standirname = os.path.join(mainstandirname, fldr)\n",
    "    Tlower_ = int(fldr.split(\"_window_\")[-1].split(\"-\")[0])\n",
    "    Tupper_ = int(fldr.split(\"_window_\")[-1].split(\"-\")[-1].split(\"_\")[0])\n",
    "    idata_chn = az.from_cmdstanpy(cmdstan.from_csv(path=standirname))\n",
    "    df_stats_ = get_stats(idata_chn.posterior, ['R0', 'doubling_time', 'reportedJul', 'reportedAug']) \n",
    "    df_stats_['Tlower'] = Tlower_\n",
    "    df_stats_['Tupper'] = Tupper_\n",
    "    df_stats_ = df_stats_.drop(['ess_bulk', 'ess_tail', 'r_hat'], axis=1)\n",
    "    Df_stats_only_sympt = df_stats_ if Df_stats is None else pd.concat([Df_stats_only_sympt, df_stats_], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d436ebe-c351-4668-b6aa-ec40aa3c3836",
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_stats_only_sympt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1846be-a651-4cc1-8b57-0ad40cfcc8b9",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">3a. Selected national outbreaks of 2022: varying the exp. window</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e60a60c-663f-4d13-aaea-28d57ad08a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_code_exp_growth_global = \"\"\"functions {\n",
    "    real gengamma_cdf(real x, real q, real mu, real sigma) {\n",
    "        real logx = log(x),\n",
    "            z = (logx - mu) / sigma,\n",
    "            a = inv_square(q),\n",
    "            value = gamma_cdf(a * exp(q * z) | a, 1);\n",
    "\n",
    "        return value;\n",
    "    }\n",
    "\n",
    "    /* discretized version */\n",
    "    vector dgengamma(real q, real mu, real sigma, int D) {\n",
    "        vector[D] res;\n",
    "        for (k in 1:D)\n",
    "            res[k] = gengamma_cdf(k - 0.5 | q, mu, sigma);\n",
    "\n",
    "        if (D > 1)\n",
    "            return append_row(append_row(res[1], tail(res, D-1) - head(res, D-1)), 1.0 - res[D]);\n",
    "        else \n",
    "            return to_vector({res[1], 1 - res[1]});\n",
    "    }\n",
    "\n",
    "    vector dgamma(real param1, real param2, int K) {\n",
    "        vector[K] res;\n",
    "        for (k in 1:K)\n",
    "            res[k] = gamma_cdf(k - 0.5 | param1, param2);\n",
    "\n",
    "        return append_row(res[1], tail(res, K-1) - head(res, K-1));\n",
    "    }\n",
    "}\n",
    "\n",
    "data {\n",
    "    int<lower = 1> Tupper; // the cutoffday for the estimation of the exponential growth \n",
    "    array[Tupper] int<lower = 0> cases_onset; // number of cases by date of symptom onset starting to be recorded till the day Tupper\n",
    "\n",
    "    int<lower = Tupper> T; // total number of days for which we have the data on number of cases by day of notification (their symptom onset date is missed)\n",
    "    array[T] int<lower = 0> cases_reported, cases_diagnosis;\n",
    "\n",
    "    // reporting delay described by the generalized gamma distribution\n",
    "    real loga, logsigma, mu;\n",
    "\n",
    "    // delay b/w onset and diagnosis described by the generalized gamma distribution\n",
    "    real loga_diagnosis, logsigma_diagnosis, mu_diagnosis;\n",
    "\n",
    "    // generation time (scale and shape of the Gamma distribution estimated in Guzetta et al. 2022)\n",
    "    real<lower = 0> genalpha, geninvbeta;\n",
    "}\n",
    "\n",
    "transformed data {\n",
    "    // reporting delay\n",
    "    real a = exp(loga), q = inv_sqrt(a), sigma = exp(logsigma);\n",
    "\n",
    "    // delay b/w onset and diagnosis\n",
    "    real a_diagnosis = exp(loga_diagnosis), q_diagnosis = inv_sqrt(a_diagnosis), sigma_diagnosis = exp(logsigma_diagnosis);\n",
    "\n",
    "    // generation time\n",
    "    real genmean = genalpha * geninvbeta,\n",
    "        gensigma = sqrt(genalpha) * geninvbeta;\n",
    "\n",
    "    // backprojecting from the reporting date\n",
    "    array[T] int cases_onset_backprj = rep_array(0, T);\n",
    "    for (t in 1:T) \n",
    "        if (cases_reported[t] > 0) {\n",
    "            vector[t+1] probs_for_backprojection = dgengamma(q, mu, sigma, t);\n",
    "            array[t+1] int counts_backprj = multinomial_rng(probs_for_backprojection, cases_reported[t]);\n",
    "            for (i in 1:t) \n",
    "                cases_onset_backprj[t-i+1] += counts_backprj[i];\n",
    "        }\n",
    "\n",
    "    // backprojecting from the diagnosis date\n",
    "    for (t in 1:T) \n",
    "        if (cases_diagnosis[t] > 0) {\n",
    "            vector[t+1] probs_for_backprojection = dgengamma(q_diagnosis, mu_diagnosis, sigma_diagnosis, t);\n",
    "            array[t+1] int counts_backprj = multinomial_rng(probs_for_backprojection, cases_diagnosis[t]);\n",
    "            for (i in 1:t) \n",
    "                cases_onset_backprj[t-i+1] += counts_backprj[i];\n",
    "        }\n",
    "\n",
    "\n",
    "    array[Tupper] int cases;\n",
    "    for (t in 1:Tupper) \n",
    "        cases[t] = cases_onset[t] + cases_onset_backprj[t];\n",
    "\n",
    "    print(cases);\n",
    "    real jitter = 1e-9;\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    // exponential growth rate\n",
    "    real logr;\n",
    "    // initial incidence\n",
    "    real<lower = 0> i0;\n",
    "    // process error\n",
    "    real<lower = 0> phi;\n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "    real r = exp(logr);\n",
    "}\n",
    "\n",
    "model {\n",
    "    logr ~ std_normal();\n",
    "    i0 ~ normal(5, 10);\n",
    "    phi ~ gamma(1, 1);\n",
    "\n",
    "    for (t in 1:Tupper)\n",
    "        if (cases[t] > 0)\n",
    "            target += neg_binomial_2_lupmf(cases[t] | i0 * exp(r * t) + jitter, phi); \n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    // basic reproduction number\n",
    "    real R0_norm_approx = exp(r * genmean - 0.5 * square(r) * square(gensigma)),\n",
    "        R0 = pow(1 + r * geninvbeta, genalpha);\n",
    "\n",
    "    // doubling time\n",
    "    real doubling_time = log(2) / r; \n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c25780-9ed9-4d4f-ae5c-5d3bf187377b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fls = !ls ../../data/WHO | grep ^epicurve_\n",
    "fls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc3066c-dc1f-4165-8591-bfbf0d4a370e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "starting_date_global = pd.to_datetime('2022-01-01', format='%Y-%m-%d')\n",
    "ending_date_global = pd.to_datetime('2022-12-31', format='%Y-%m-%d')\n",
    "\n",
    "df_cumcases = None\n",
    "for fl_ in fls:\n",
    "    df_ = pd.read_csv(os.path.join('../../data/WHO', fl_))\n",
    "    df_['reference_date'] = pd.to_datetime(df_.reference_date, format='%Y-%m-%d')\n",
    "    df_ = df_.loc[lambda d: d.reference_date <= ending_date_global]\n",
    "    country_ = fl_.split('_')[1][:-4]\n",
    "    who_region_ = df_.who_region.values[0]\n",
    "    print(country_)\n",
    "    if len(df_.loc[lambda d: d.date_type=='Onset']) > 0:\n",
    "        df_mxs_ = df_.loc[lambda d: d.date_type=='Onset']\n",
    "        df_mxs_ = df_mxs_.loc[lambda d: d.cases==np.max(df_mxs_.cases)]\n",
    "        df_mxs_ = df_mxs_.loc[lambda d: d.reference_date==np.min(df_mxs_.reference_date)]\n",
    "        df_cumcases_ = pd.DataFrame({'country': [country_], 'who_region': who_region_, 'cases': [df_['cases'].sum()], \n",
    "                                     'peak_cases': df_mxs_.cases, 'peak_date': df_mxs_.reference_date, 'peak_date_type': df_mxs_.date_type, \n",
    "                                     'file': fl_})\n",
    "        df_cumcases = df_cumcases_ if df_cumcases is None else pd.concat([df_cumcases, df_cumcases_], ignore_index=True)\n",
    "df_cumcases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2543a67e-5088-4c96-bf84-177a5dc1e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if no selection by symptomatic, then +Ireland 227 cases\n",
    "df_cumcases_selection = df_cumcases.loc[lambda d: d.cases>700]\n",
    "print(\"number of countries: \", len(df_cumcases_selection))\n",
    "df_cumcases_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df44c4b7-d83a-4c8a-8d68-9e46fc93cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_country(idx, Tupper_):\n",
    "    df_sel_ = df_cumcases_selection.iloc[idx]\n",
    "    print(df_sel_)\n",
    "    \n",
    "    basename = truncation_date_hk.strftime(\"%Y%m%d\")+f'_Tupper-{Tupper_}_exp_growth_Miura2023_' + df_sel_.country\n",
    "    standirname = os.path.join(mainstandirname, basename)\n",
    "    !rm {standirname}/*\n",
    "    os.makedirs(standirname, exist_ok=True)\n",
    "    stanscriptdir = '../Dropbox/'+standirname[9:]\n",
    "\n",
    "    # Miura et al. 2023\n",
    "    mean_gt_ = 10.1; sd_gt_ = 6.1\n",
    "    genalpha_ = (mean_gt_ / sd_gt_)**2\n",
    "    geninvbeta_ = (sd_gt_**2) / mean_gt_ \n",
    "\n",
    "    fl_ = df_sel_['file']\n",
    "    df_ = pd.read_csv(os.path.join('../../data/WHO', fl_))\n",
    "    df_['reference_date'] = pd.to_datetime(df_.reference_date, format='%Y-%m-%d')\n",
    "    df_ = df_.loc[lambda d: d.reference_date<=ending_date_global]\n",
    "\n",
    "    df_country_ = df_.groupby(['date_type', 'reference_date'])['cases'].sum().reset_index().pivot_table(values='cases', columns='date_type', index='reference_date')\n",
    "    df_country_ = pd.DataFrame(df_country_.to_records()).merge(pd.DataFrame({'reference_date': pd.date_range(starting_date_global, ending_date_global)}), how='outer').fillna(0)\n",
    "    df_country_ = df_country_.sort_values('reference_date').set_index('reference_date').astype('int64')\n",
    "    df_country_['reference_day'] = (df_country_.index - starting_date_global).days\n",
    "    df_ = df_country_\n",
    "\n",
    "    T_ = df_.shape[0]\n",
    "    cases_onset_ = df_.loc[lambda d: (d.index>df_sel_.peak_date-pd.DateOffset(days=Tupper_))].Onset.values[:Tupper_]\n",
    "    cases_reported_ = df_['Reported'].astype('int64').values if 'Reported' in df_.columns else [0]*T_\n",
    "    cases_diagnosis_ = df_['Diagnosis'].astype('int64').values if 'Diagnosis' in df_.columns else [0]*T_\n",
    "\n",
    "    stan_data = dict({\n",
    "        'Tupper': Tupper_,\n",
    "        'cases_onset': cases_onset_,\n",
    "        'T': df_.shape[0],\n",
    "        'cases_reported': cases_reported_,\n",
    "        'loga': stats_summary_rep_delay.loc['loga']['mean'],\n",
    "        'logsigma': stats_summary_rep_delay.loc['logsigma']['mean'],\n",
    "        'mu': stats_summary_rep_delay.loc['mu']['mean'],\n",
    "        'cases_diagnosis': cases_diagnosis_,\n",
    "        'loga_diagnosis': stats_summary_rep_delay_diagnosis.loc['loga']['mean'],\n",
    "        'logsigma_diagnosis': stats_summary_rep_delay_diagnosis.loc['logsigma']['mean'],\n",
    "        'mu_diagnosis': stats_summary_rep_delay_diagnosis.loc['mu']['mean'],\n",
    "        'genalpha': genalpha_,\n",
    "        'geninvbeta': geninvbeta_\n",
    "    })\n",
    "    stan_data_file = os.path.join(standirname, 'Data.json')\n",
    "    cmdstan.write_stan_json(stan_data_file, stan_data)\n",
    "\n",
    "    stan_inits = dict({\n",
    "        'r': 0.1,\n",
    "        'phi': 1.0\n",
    "    })\n",
    "    stan_init_file = os.path.join(standirname, 'Inits.json')\n",
    "    cmdstan.write_stan_json(stan_init_file, stan_inits)\n",
    "\n",
    "    stan_code_file = os.path.join(standirname, f'fit_exp_growth.stan')\n",
    "    with open(stan_code_file, \"w+\") as f:\n",
    "        f.write(stan_code_exp_growth_global)\n",
    "        f.close()\n",
    "    \n",
    "    model = cmdstan.CmdStanModel(stan_file=stan_code_file, cpp_options={'STAN_THREADS': 'TRUE'}, compile='force')\n",
    "    fit = model.sample(data=stan_data_file, seed = 1, iter_warmup=num_warmup, iter_sampling=1, inits=stan_init_file, parallel_chains=10,\n",
    "                       show_console=False, show_progress=False, chains = 4000)\n",
    "    fit.save_csvfiles(dir=standirname)\n",
    "\n",
    "    idata = az.from_cmdstanpy(posterior=fit)\n",
    "    df_stats = get_stats(idata.posterior, ['r', 'i0', 'phi', 'R0', 'doubling_time']) \n",
    "    df_stats['country'] = df_sel_.country\n",
    "    df_stats['who_region'] = df_sel_.who_region\n",
    "    df_stats['expwindow'] = Tupper_\n",
    "\n",
    "    return(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a1d4b-a936-4146-b095-7d902d1f6143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "df_stats_countries = None\n",
    "for Tupper_ in [30, 60]:\n",
    "    for idx in range(len(df_cumcases_selection)):\n",
    "        df_stats_ = sim_country(idx, Tupper_)\n",
    "        df_stats_countries = df_stats_ if df_stats_countries is None else pd.concat([df_stats_countries, df_stats_], ignore_index=True)\n",
    "df_stats_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd90295-7f67-4020-b2a3-4aa1a3187f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_countries['SD'] = [(upper - lower) / 1.96 / 2 for lower, upper in zip(df_stats_countries['q2.5'], df_stats_countries['q97.5'])]\n",
    "df_stats_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4ff2e6-6a61-48bd-acb5-97c4217e6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_code_meta_summary = \"\"\"data {\n",
    "  int<lower=1> J;  // number of studies with available data\n",
    "  vector[J] mu_known;  // means of known studies\n",
    "  vector<lower=0>[J] stderr_known;  // standard errors of known studies\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real<lower=0> mu, tau_squared;\n",
    "}\n",
    "\n",
    "model {\n",
    "  // Priors\n",
    "  mu ~ normal(4, 8);  // weakly informative prior for the overall mean\n",
    "  tau_squared ~ cauchy(0, 5);  // weakly informative prior for the between-study variability\n",
    "\n",
    "  // Likelihood\n",
    "  mu_known ~ normal(mu, sqrt(square(stderr_known) + tau_squared));\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "  real mu_pred;\n",
    "\n",
    "    {\n",
    "        // Predict a future observation for a hypothetical new study\n",
    "        mu_pred = normal_rng(mu, sqrt(tau_squared));\n",
    "    }\n",
    "}\"\"\"\n",
    "\n",
    "Df_meta_stats = None\n",
    "for Tupper_ in [30, 60]:\n",
    "    basename = f'R0_meta_Tupper-{Tupper_}_Miura2023'\n",
    "    standirname = os.path.join(mainstandirname, basename)\n",
    "    if recalc_everything:\n",
    "        !rm {standirname}/*\n",
    "        os.makedirs(standirname, exist_ok=True)\n",
    "        stanscriptdir = '../Dropbox/'+standirname[9:]\n",
    "            \n",
    "        stan_code_file = os.path.join(standirname, 'fit_R0_meta.stan')\n",
    "        with open(stan_code_file, \"w+\") as f:\n",
    "            f.write(stan_code_meta_summary)\n",
    "            f.close()\n",
    "    \n",
    "        df_ = df_stats_countries.copy().loc[lambda d: (d['var']=='R0')&(d['expwindow']==Tupper_)][::-1]\n",
    "        df_ = df_.loc[lambda d: ~((d.who_region=='AMRO')&(d.country!='united states of america')&(d.country!='canada'))]\n",
    "        \n",
    "        stan_data = dict({\n",
    "            'J': df_.shape[0],\n",
    "            'mu_known': df_['mean'].values,\n",
    "            'stderr_known': df_['SD'].values\n",
    "        })\n",
    "        stan_data_file = os.path.join(standirname, 'Data.json')\n",
    "        cmdstan.write_stan_json(stan_data_file, stan_data)\n",
    "        \n",
    "        def bash_file(stanscriptdir):\n",
    "            return f\"\"\"#!/bin/bash\n",
    "cwd=$(pwd)\n",
    "cd {standistribdir_}\n",
    "make -j4 {stanscriptdir}/fit_R0_meta\n",
    "cd {stanscriptdir}\n",
    "mkdir -p diagnostics\n",
    "for i in {{1..4}}\n",
    "do\n",
    "    echo Running ${{i}}\n",
    "    SEEDNUMBER=$((1+$i))\n",
    "    ./fit_R0_meta \\\\\n",
    "        method=sample num_samples={num_iterations} num_warmup={num_warmup} thin=1 save_warmup=0 adapt delta=0.98 \\\\\n",
    "            algorithm=hmc \\\\\n",
    "                engine=nuts \\\\\n",
    "        random seed=${{SEEDNUMBER}} \\\\\n",
    "        id=$i \\\\\n",
    "        data file=Data.json \\\\\n",
    "        output file=trace-$i.csv \\\\\n",
    "            diagnostic_file=diagnostics/diagnostics-$i.csv > diagnostics/output-$i.txt &\n",
    "done\n",
    "echo Finished haha!\n",
    "\"\"\"            \n",
    "        model = cmdstan.CmdStanModel(stan_file=stan_code_file)\n",
    "        fit = model.sample(data=stan_data_file, seed = 1, iter_warmup=num_warmup, iter_sampling=1000,\n",
    "                           show_console=False, show_progress=True, chains = 4)\n",
    "        fit.save_csvfiles(dir=standirname)\n",
    "\n",
    "        idata_meta = az.from_cmdstanpy(posterior=fit)\n",
    "        df_meta_stats = get_stats(idata_meta.posterior, ['mu', 'tau_squared'])\n",
    "        df_meta_stats['expwindow'] = Tupper_\n",
    "        Df_meta_stats = df_meta_stats if Df_meta_stats is None else pd.concat([Df_meta_stats, df_meta_stats], ignore_index = True)\n",
    "Df_meta_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcebc539-e07e-436d-a946-7aa7ca34190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_countries.merge(df_cumcases_selection.loc[:, ['country', 'cases', 'peak_cases', 'peak_date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9346112-9c84-4fa9-a4ab-97c697ff1f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Tupper_ in [30, 60]:\n",
    "    fig = plt.figure(figsize = [5.5, 4.5]) \n",
    "    ax1 = fig.add_subplot()\n",
    "    \n",
    "    df_ = df_stats_countries.copy().merge(df_cumcases_selection.loc[:, ['country', 'cases', 'peak_cases', 'peak_date']])\\\n",
    "        .loc[lambda d: (d['var']=='R0')&(d['expwindow']==Tupper_)][::-1]\n",
    "    \n",
    "    df_['color'] = ['lightgrey' if (who_region=='AMRO')&(country!='united states of america')&(country!='canada') else 'k' for country, who_region in zip(df_.country,df_.who_region)]\n",
    "    df_ = df_.sort_values('color')\n",
    "    \n",
    "    df_['Source'] = [x.title().replace(' Of ', ' of ') for x in df_['country']]\n",
    "    df_['Source_y'] = [(i+1)/df_.shape[0] for i in range(df_.shape[0])]\n",
    "    \n",
    "    for clr_ in df_['color'].drop_duplicates().values:\n",
    "        df__ = df_.loc[lambda d: d['color']==clr_]\n",
    "        lsty = 'dashed' if clr_=='lightgrey' else 'solid'\n",
    "        eb1 = ax1.errorbar(x=list(df__['median']), y=df__['Source_y'], \n",
    "                     xerr=[list(df__['median'] - df__['q2.5']), list(df__['q97.5'] - df__['median'])], color='k', capsize=2,\n",
    "                     linestyle='None', linewidth=1, markersize=0)\n",
    "        if clr_=='lightgrey':\n",
    "            eb1[-1][0].set_linestyle('--')\n",
    "            clr0_ = 'w'\n",
    "        else:\n",
    "            clr0_ = 'k'\n",
    "        ax1.plot(list(df__['median']), df__['Source_y'], color='k', mfc=clr0_, \n",
    "                 linestyle='None', linewidth=1, marker=\"o\", ms=5)\n",
    "    \n",
    "    # Pooled mean\n",
    "    mean_meta = Df_meta_stats.loc[lambda d: d['expwindow']==Tupper_].loc[lambda d: d['var']=='mu', 'mean'].values[0]\n",
    "    lower_meta = Df_meta_stats.loc[lambda d: d['expwindow']==Tupper_].loc[lambda d: d['var']=='mu', 'q2.5'].values[0]\n",
    "    upper_meta = Df_meta_stats.loc[lambda d: d['expwindow']==Tupper_].loc[lambda d: d['var']=='mu', 'q97.5'].values[0]\n",
    "    sqr_meta = Df_meta_stats.loc[lambda d: d['expwindow']==Tupper_].loc[lambda d: d['var']=='tau_squared', 'mean'].values[0]\n",
    "    y_ = -0.15/df_.shape[0]\n",
    "    h_ = 0.3/df_.shape[0]\n",
    "    from matplotlib.patches import Polygon\n",
    "    pts = np.array([[lower_meta, y_], [mean_meta,y_+h_], [upper_meta, y_], [mean_meta, y_-h_], [lower_meta, y_]])\n",
    "    p = Polygon(pts, closed=False, color='k')\n",
    "    ax1.add_patch(p)\n",
    "    \n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.set_xlim(left = 0.8, right=3.0)\n",
    "    ax1.set_xticks(np.arange(1, 3.5, .5))\n",
    "    ax1.get_xaxis().tick_bottom()\n",
    "    ax1.get_yaxis().tick_left()\n",
    "    \n",
    "    ax1.set_ylim(-1/df_.shape[0], np.max(df_['Source_y'])+.02)\n",
    "    ax1.set_xlabel(\"Effective reproduction number\")\n",
    "    ax1.set_yticks(np.r_[df_['Source_y'], y_])\n",
    "    ax1.set_yticklabels(np.r_[df_['Source'], [f'Pooled mean,  = {sqr_meta:.2f}']] )\n",
    "    \n",
    "    ax2 = ax1.secondary_yaxis(\"right\")\n",
    "    ax2.set_yticks(ax1.get_yticks())\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.tick_params(length=0)\n",
    "    df_['label'] = [f\"{mu:.2f} ({mu_lower:.2f}{mu_upper:.2f})\" for mu, mu_lower, mu_upper in zip(df_['mean'], df_['q2.5'], df_['q97.5'])]\n",
    "    ax2.set_yticklabels(np.r_[df_['label'], [f'{mean_meta:.2f} ({lower_meta:.2f}{upper_meta:.2f})']], ha='center', fontsize=9)\n",
    "    ax2.yaxis.set_tick_params(pad=45)\n",
    "\n",
    "    ax3 = ax1.secondary_yaxis(\"right\")\n",
    "    ax3.set_yticks(ax1.get_yticks())\n",
    "    ax3.spines['right'].set_visible(False)\n",
    "    ax3.tick_params(length=0)\n",
    "    df_['label2'] = [f\"{(peak_date-pd.DateOffset(days=Tupper_)).strftime('%m/%d')}-{peak_date.strftime('%m/%d')}\" for peak_date in df_['peak_date']]\n",
    "    ax3.set_yticklabels(np.r_[df_['label2'], ['']], ha='center', fontsize=9)\n",
    "    ax3.yaxis.set_tick_params(pad=125)\n",
    "    \n",
    "    [label.set_fontweight('bold') if 'Pool' in label.get_text() else None for label in ax1.get_yticklabels()]\n",
    "    [label2.set_fontweight('bold') if 'Pool' in label.get_text() else None for label, label2 in zip(ax1.get_yticklabels(),ax2.get_yticklabels())]\n",
    "    \n",
    "    [label.set_color('k') if 'Pool' in label.get_text() else None for label in ax1.get_yticklabels()]\n",
    "    [label2.set_color('k') if 'Pool' in label.get_text() else None for label, label2 in zip(ax1.get_yticklabels(),ax2.get_yticklabels())]\n",
    "    \n",
    "    [label.set_color(clr_chn_) if 'China' in label.get_text() else None for label in ax1.get_yticklabels()]\n",
    "    [label2.set_color(clr_chn_) if 'China' in label.get_text() else None for label, label2 in zip(ax1.get_yticklabels(),ax2.get_yticklabels())]\n",
    "    \n",
    "    plt.text(.44, 1.07, 'Country', horizontalalignment='left', fontsize=10)\n",
    "    plt.text(3.59, 1.07, 'Mean (95% CrI)', horizontalalignment='right', fontsize=10)\n",
    "    plt.text(3.89, 1.05, 'Period\\n(m/d in 2022)', horizontalalignment='center', fontsize=10)\n",
    "    \n",
    "    plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "    if save_figures:\n",
    "        plt.savefig(f\"../../figures/fig-R0_meta_Tupper-{Tupper_}_Miura2023.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c1dea-97b3-45bb-8d97-60ca8a19c651",
   "metadata": {},
   "source": [
    "## <font color=\"orange\">3b. Selected national outbreaks of 2022: only symptomatic cases</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ba0aa-813f-4409-b0b6-76e989f80d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_code_exp_growth_global_only_symptomatic = \"\"\"functions {\n",
    "    real gengamma_cdf(real x, real q, real mu, real sigma) {\n",
    "        real logx = log(x),\n",
    "            z = (logx - mu) / sigma,\n",
    "            a = inv_square(q),\n",
    "            value = gamma_cdf(a * exp(q * z) | a, 1);\n",
    "\n",
    "        return value;\n",
    "    }\n",
    "\n",
    "    /* discretized version */\n",
    "    vector dgengamma(real q, real mu, real sigma, int D) {\n",
    "        vector[D] res;\n",
    "        for (k in 1:D)\n",
    "            res[k] = gengamma_cdf(k - 0.5 | q, mu, sigma);\n",
    "\n",
    "        if (D > 1)\n",
    "            return append_row(append_row(res[1], tail(res, D-1) - head(res, D-1)), 1.0 - res[D]);\n",
    "        else \n",
    "            return to_vector({res[1], 1 - res[1]});\n",
    "    }\n",
    "\n",
    "    vector dgamma(real param1, real param2, int K) {\n",
    "        vector[K] res;\n",
    "        for (k in 1:K)\n",
    "            res[k] = gamma_cdf(k - 0.5 | param1, param2);\n",
    "\n",
    "        return append_row(res[1], tail(res, K-1) - head(res, K-1));\n",
    "    }\n",
    "}\n",
    "\n",
    "data {\n",
    "    int<lower = 1> Tupper; // the cutoffday for the estimation of the exponential growth \n",
    "    array[Tupper] int<lower = 0> cases_onset; // number of cases by date of symptom onset starting to be recorded till the day Tupper\n",
    "    \n",
    "    // generation time (scale and shape of the Gamma distribution estimated in Guzetta et al. 2022)\n",
    "    real<lower = 0> genalpha, geninvbeta;\n",
    "}\n",
    "\n",
    "transformed data {\n",
    "    // generation time\n",
    "    real genmean = genalpha * geninvbeta,\n",
    "        gensigma = sqrt(genalpha) * geninvbeta;\n",
    "        \n",
    "    real jitter = 1e-9;\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    // exponential growth rate\n",
    "    real logr;\n",
    "    // initial incidence\n",
    "    real<lower = 0> i0;\n",
    "    // process error\n",
    "    real<lower = 0> phi;\n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "    real r = exp(logr);\n",
    "}\n",
    "\n",
    "model {\n",
    "    logr ~ std_normal();\n",
    "    i0 ~ normal(5, 10);\n",
    "    phi ~ gamma(1, 1);\n",
    "\n",
    "    for (t in 1:Tupper)\n",
    "        if (cases_onset[t] > 0)\n",
    "            target += neg_binomial_2_lupmf(cases_onset[t] | i0 * exp(r * t) + jitter, phi); \n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    // basic reproduction number\n",
    "    real R0_norm_approx = exp(r * genmean - 0.5 * square(r) * square(gensigma)),\n",
    "        R0 = pow(1 + r * geninvbeta, genalpha);\n",
    "\n",
    "    // doubling time\n",
    "    real doubling_time = log(2) / r; \n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd9e83-ada2-4533-b39d-c19ea2efe2e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fls = !ls ../../data/WHO | grep ^epicurve_\n",
    "fls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1d4375-15f0-4705-bd89-20e12409eeac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "starting_date_global = pd.to_datetime('2022-01-01', format='%Y-%m-%d')\n",
    "ending_date_global = pd.to_datetime('2022-12-31', format='%Y-%m-%d')\n",
    "\n",
    "df_cumcases = None\n",
    "for fl_ in fls:\n",
    "    df_ = pd.read_csv(os.path.join('../../data/WHO', fl_))\n",
    "    df_['reference_date'] = pd.to_datetime(df_.reference_date, format='%Y-%m-%d')\n",
    "    df_ = df_.loc[lambda d: d.reference_date<=ending_date_global]\n",
    "    country_ = fl_.split('_')[1][:-4]\n",
    "    who_region_ = df_.who_region.values[0]\n",
    "    print(country_)\n",
    "    if len(df_.loc[lambda d: d.date_type=='Onset']) > 0:\n",
    "        df_mxs_ = df_.loc[lambda d: d.date_type=='Onset']\n",
    "        df_mxs_ = df_mxs_.loc[lambda d: d.cases==np.max(df_mxs_.cases)]\n",
    "        df_mxs_ = df_mxs_.loc[lambda d: d.reference_date==np.min(df_mxs_.reference_date)]\n",
    "        df_cumcases_ = pd.DataFrame({'country': [country_], 'who_region': who_region_, 'cases': [df_['cases'].sum()], \n",
    "                                     'peak_cases': df_mxs_.cases, 'peak_date': df_mxs_.reference_date, 'peak_date_type': df_mxs_.date_type, \n",
    "                                     'file': fl_})\n",
    "        df_cumcases = df_cumcases_ if df_cumcases is None else pd.concat([df_cumcases, df_cumcases_], ignore_index=True)\n",
    "df_cumcases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f668f-13ca-4213-af12-229f58daae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if no selection by symptomatic, then +Ireland 227 cases\n",
    "df_cumcases_selection = df_cumcases.loc[lambda d: d.cases>700]\n",
    "print(\"number of countries: \", len(df_cumcases_selection))\n",
    "df_cumcases_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bbb25c-82a2-430d-9551-11547b4c8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_country_only_sympt(idx, Tupper_):\n",
    "    df_sel_ = df_cumcases_selection.iloc[idx]\n",
    "    print(df_sel_)\n",
    "    \n",
    "    basename = truncation_date_hk.strftime(\"%Y%m%d\")+f'_Tupper-{Tupper_}_exp_growth_only_sympt_Miura2023_' + df_sel_.country\n",
    "    standirname = os.path.join(mainstandirname, basename)\n",
    "    !rm {standirname}/*\n",
    "    os.makedirs(standirname, exist_ok=True)\n",
    "    stanscriptdir = '../Dropbox/'+standirname[9:]\n",
    "\n",
    "    # Miura et al. 2023\n",
    "    mean_gt_ = 10.1; sd_gt_ = 6.1\n",
    "    genalpha_ = (mean_gt_ / sd_gt_)**2\n",
    "    geninvbeta_ = (sd_gt_**2) / mean_gt_ \n",
    "\n",
    "    fl_ = df_sel_['file']\n",
    "    df_ = pd.read_csv(os.path.join('../../data/WHO', fl_))\n",
    "    df_['reference_date'] = pd.to_datetime(df_.reference_date, format='%Y-%m-%d')\n",
    "    df_ = df_.loc[lambda d: d.reference_date<=ending_date_global]\n",
    "\n",
    "    df_country_ = df_.groupby(['date_type', 'reference_date'])['cases'].sum().reset_index().pivot_table(values='cases', columns='date_type', index='reference_date')\n",
    "    df_country_ = pd.DataFrame(df_country_.to_records()).merge(pd.DataFrame({'reference_date': pd.date_range(starting_date_global, ending_date_global)}), how='outer').fillna(0)\n",
    "    df_country_ = df_country_.sort_values('reference_date').set_index('reference_date').astype('int64')\n",
    "    df_country_['reference_day'] = (df_country_.index - starting_date_global).days\n",
    "    df_ = df_country_\n",
    "\n",
    "    T_ = df_.shape[0]\n",
    "    cases_onset_ = df_.loc[lambda d: (d.index>df_sel_.peak_date-pd.DateOffset(days=Tupper_))].Onset.values[:Tupper_]\n",
    "    cases_reported_ = df_['Reported'].astype('int64').values if 'Reported' in df_.columns else [0]*T_\n",
    "    cases_diagnosis_ = df_['Diagnosis'].astype('int64').values if 'Diagnosis' in df_.columns else [0]*T_\n",
    "\n",
    "    stan_data = dict({\n",
    "        'Tupper': Tupper_,\n",
    "        'cases_onset': cases_onset_,\n",
    "        'genalpha': genalpha_,\n",
    "        'geninvbeta': geninvbeta_\n",
    "    })\n",
    "    stan_data_file = os.path.join(standirname, 'Data.json')\n",
    "    cmdstan.write_stan_json(stan_data_file, stan_data)\n",
    "\n",
    "    stan_inits = dict({\n",
    "        'r': 0.1,\n",
    "        'phi': 1.0\n",
    "    })\n",
    "    stan_init_file = os.path.join(standirname, 'Inits.json')\n",
    "    cmdstan.write_stan_json(stan_init_file, stan_inits)\n",
    "\n",
    "    stan_code_file = os.path.join(standirname, f'fit_exp_growth.stan')\n",
    "    with open(stan_code_file, \"w+\") as f:\n",
    "        f.write(stan_code_exp_growth_global_only_symptomatic)\n",
    "        f.close()\n",
    "    \n",
    "    model = cmdstan.CmdStanModel(stan_file=stan_code_file, cpp_options={'STAN_THREADS': 'TRUE'}, compile='force')\n",
    "    fit = model.sample(data=stan_data_file, seed = 1, iter_warmup=num_warmup, iter_sampling=1, inits=stan_init_file, parallel_chains=10,\n",
    "                       show_console=False, show_progress=False, chains = 4000)\n",
    "    fit.save_csvfiles(dir=standirname)\n",
    "\n",
    "    idata = az.from_cmdstanpy(posterior=fit)\n",
    "    df_stats = get_stats(idata.posterior, ['r', 'i0', 'phi', 'R0', 'doubling_time']) \n",
    "    df_stats['country'] = df_sel_.country\n",
    "    df_stats['who_region'] = df_sel_.who_region\n",
    "    df_stats['expwindow'] = Tupper_\n",
    "\n",
    "    return(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1286d-b59b-42b4-a205-95374282e138",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "df_stats_countries = None\n",
    "for Tupper_ in [45, 30, 60]:\n",
    "    for idx in range(len(df_cumcases_selection)):\n",
    "        df_stats_ = sim_country_only_sympt(idx, Tupper_)\n",
    "        df_stats_countries = df_stats_ if df_stats_countries is None else pd.concat([df_stats_countries, df_stats_], ignore_index=True)\n",
    "df_stats_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50a656-2c70-4b22-9dab-f5b20e398efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbabff5-5385-4485-aea5-be6e2dba96d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats_countries['SD'] = [(upper - lower) / 1.96 / 2 for lower, upper in zip(df_stats_countries['q2.5'], df_stats_countries['q97.5'])]\n",
    "df_stats_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d27566d-13f1-4620-a9a3-332e7858da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_code_meta_summary = \"\"\"data {\n",
    "  int<lower=1> J;  // number of studies with available data\n",
    "  vector[J] mu_known;  // means of known studies\n",
    "  vector<lower=0>[J] stderr_known;  // standard errors of known studies\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real<lower=0> mu, tau_squared;\n",
    "}\n",
    "\n",
    "model {\n",
    "  // Priors\n",
    "  mu ~ normal(4, 8);  // weakly informative prior for the overall mean\n",
    "  tau_squared ~ cauchy(0, 5);  // weakly informative prior for the between-study variability\n",
    "\n",
    "  // Likelihood\n",
    "  mu_known ~ normal(mu, sqrt(square(stderr_known) + tau_squared));\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "  real mu_pred;\n",
    "\n",
    "    {\n",
    "        // Predict a future observation for a hypothetical new study\n",
    "        mu_pred = normal_rng(mu, sqrt(tau_squared));\n",
    "    }\n",
    "}\"\"\"\n",
    "\n",
    "Df_meta_stats = None\n",
    "for Tupper_ in [45, 30, 60]:\n",
    "    basename = f'R0_meta_only_sympt_Miura2023_Tupper-{Tupper_}'\n",
    "    standirname = os.path.join(mainstandirname, basename)\n",
    "    if recalc_everything:\n",
    "        !rm {standirname}/*\n",
    "        os.makedirs(standirname, exist_ok=True)\n",
    "        stanscriptdir = '../Dropbox/'+standirname[9:]\n",
    "            \n",
    "        stan_code_file = os.path.join(standirname, 'fit_R0_meta.stan')\n",
    "        with open(stan_code_file, \"w+\") as f:\n",
    "            f.write(stan_code_meta_summary)\n",
    "            f.close()\n",
    "    \n",
    "        df_ = df_stats_countries.copy().loc[lambda d: (d['var']=='R0')&(d['expwindow']==Tupper_)][::-1]\n",
    "        df_ = df_.loc[lambda d: ~((d.who_region=='AMRO')&(d.country!='united states of america')&(d.country!='canada'))]\n",
    "        \n",
    "        stan_data = dict({\n",
    "            'J': df_.shape[0],\n",
    "            'mu_known': df_['mean'].values,\n",
    "            'stderr_known': df_['SD'].values\n",
    "        })\n",
    "        stan_data_file = os.path.join(standirname, 'Data.json')\n",
    "        cmdstan.write_stan_json(stan_data_file, stan_data)\n",
    "        \n",
    "        def bash_file(stanscriptdir):\n",
    "            return f\"\"\"#!/bin/bash\n",
    "cwd=$(pwd)\n",
    "cd {standistribdir_}\n",
    "make -j4 {stanscriptdir}/fit_R0_meta\n",
    "cd {stanscriptdir}\n",
    "mkdir -p diagnostics\n",
    "for i in {{1..4}}\n",
    "do\n",
    "    echo Running ${{i}}\n",
    "    SEEDNUMBER=$((1+$i))\n",
    "    ./fit_R0_meta \\\\\n",
    "        method=sample num_samples={num_iterations} num_warmup={num_warmup} thin=1 save_warmup=0 adapt delta=0.98 \\\\\n",
    "            algorithm=hmc \\\\\n",
    "                engine=nuts \\\\\n",
    "        random seed=${{SEEDNUMBER}} \\\\\n",
    "        id=$i \\\\\n",
    "        data file=Data.json \\\\\n",
    "        output file=trace-$i.csv \\\\\n",
    "            diagnostic_file=diagnostics/diagnostics-$i.csv > diagnostics/output-$i.txt &\n",
    "done\n",
    "echo Finished haha!\n",
    "\"\"\"            \n",
    "        model = cmdstan.CmdStanModel(stan_file=stan_code_file)\n",
    "        fit = model.sample(data=stan_data_file, seed = 1, iter_warmup=num_warmup, iter_sampling=1000,\n",
    "                           show_console=False, show_progress=True, chains = 4)\n",
    "        fit.save_csvfiles(dir=standirname)\n",
    "\n",
    "        idata_meta = az.from_cmdstanpy(posterior=fit)\n",
    "        df_meta_stats = get_stats(idata_meta.posterior, ['mu', 'tau_squared'])\n",
    "        df_meta_stats['expwindow'] = Tupper_\n",
    "        Df_meta_stats = df_meta_stats if Df_meta_stats is None else pd.concat([Df_meta_stats, df_meta_stats], ignore_index = True)\n",
    "Df_meta_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a2bad-74b7-4866-8caa-706181e8569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Tupper_ in [45, 30, 60]:\n",
    "    fig = plt.figure(figsize = [5.5, 4.5]) \n",
    "    ax1 = fig.add_subplot()\n",
    "    \n",
    "    df_ = df_stats_countries.copy().merge(df_cumcases_selection.loc[:, ['country', 'cases', 'peak_cases', 'peak_date']])\\\n",
    "        .loc[lambda d: (d['var']=='R0')&(d['expwindow']==Tupper_)][::-1]\n",
    "    \n",
    "    df_['color'] = ['lightgrey' if (who_region=='AMRO')&(country!='united states of america')&(country!='canada') else 'k' for country, who_region in zip(df_.country,df_.who_region)]\n",
    "    df_ = df_.sort_values('color')\n",
    "    \n",
    "    df_['Source'] = [x.title().replace(' Of ', ' of ') for x in df_['country']]\n",
    "    df_['Source_y'] = [(i+1)/df_.shape[0] for i in range(df_.shape[0])]\n",
    "    \n",
    "    for clr_ in df_['color'].drop_duplicates().values:\n",
    "        df__ = df_.loc[lambda d: d['color']==clr_]\n",
    "        lsty = 'dashed' if clr_=='lightgrey' else 'solid'\n",
    "        eb1 = ax1.errorbar(x=list(df__['median']), y=df__['Source_y'], \n",
    "                     xerr=[list(df__['median'] - df__['q2.5']), list(df__['q97.5'] - df__['median'])], color='k', capsize=2,\n",
    "                     linestyle='None', linewidth=1, markersize=0)\n",
    "        if clr_=='lightgrey':\n",
    "            eb1[-1][0].set_linestyle('--')\n",
    "            clr0_ = 'w'\n",
    "        else:\n",
    "            clr0_ = 'k'\n",
    "        ax1.plot(list(df__['median']), df__['Source_y'], color='k', mfc=clr0_, \n",
    "                 linestyle='None', linewidth=1, marker=\"o\", ms=5)\n",
    "    \n",
    "    # Pooled mean\n",
    "    mean_meta = Df_meta_stats.loc[lambda d: d['expwindow']==Tupper_].loc[lambda d: d['var']=='mu', 'mean'].values[0]\n",
    "    lower_meta = Df_meta_stats.loc[lambda d: d['expwindow']==Tupper_].loc[lambda d: d['var']=='mu', 'q2.5'].values[0]\n",
    "    upper_meta = Df_meta_stats.loc[lambda d: d['expwindow']==Tupper_].loc[lambda d: d['var']=='mu', 'q97.5'].values[0]\n",
    "    sqr_meta = Df_meta_stats.loc[lambda d: d['expwindow']==Tupper_].loc[lambda d: d['var']=='tau_squared', 'mean'].values[0]\n",
    "    y_ = -0.15/df_.shape[0]\n",
    "    h_ = 0.3/df_.shape[0]\n",
    "    from matplotlib.patches import Polygon\n",
    "    pts = np.array([[lower_meta, y_], [mean_meta,y_+h_], [upper_meta, y_], [mean_meta, y_-h_], [lower_meta, y_]])\n",
    "    p = Polygon(pts, closed=False, color='k')\n",
    "    ax1.add_patch(p)\n",
    "    \n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.set_xlim(left = 0.8, right=3.0)\n",
    "    ax1.set_xticks(np.arange(1, 3.5, .5))\n",
    "    ax1.get_xaxis().tick_bottom()\n",
    "    ax1.get_yaxis().tick_left()\n",
    "    \n",
    "    ax1.set_ylim(-1/df_.shape[0], np.max(df_['Source_y'])+.02)\n",
    "    ax1.set_xlabel(\"Effective reproduction number\")\n",
    "    ax1.set_yticks(np.r_[df_['Source_y'], y_])\n",
    "    ax1.set_yticklabels(np.r_[df_['Source'], [f'Pooled mean,  = {sqr_meta:.2f}']] )\n",
    "    \n",
    "    ax2 = ax1.secondary_yaxis(\"right\")\n",
    "    ax2.set_yticks(ax1.get_yticks())\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.tick_params(length=0)\n",
    "    df_['label'] = [f\"{mu:.2f} ({mu_lower:.2f}{mu_upper:.2f})\" for mu, mu_lower, mu_upper in zip(df_['mean'], df_['q2.5'], df_['q97.5'])]\n",
    "    ax2.set_yticklabels(np.r_[df_['label'], [f'{mean_meta:.2f} ({lower_meta:.2f}{upper_meta:.2f})']], ha='center', fontsize=9)\n",
    "    ax2.yaxis.set_tick_params(pad=45)\n",
    "\n",
    "    ax3 = ax1.secondary_yaxis(\"right\")\n",
    "    ax3.set_yticks(ax1.get_yticks())\n",
    "    ax3.spines['right'].set_visible(False)\n",
    "    ax3.tick_params(length=0)\n",
    "    df_['label2'] = [f\"{(peak_date-pd.DateOffset(days=Tupper_)).strftime('%m/%d')}-{peak_date.strftime('%m/%d')}\" for peak_date in df_['peak_date']]\n",
    "    ax3.set_yticklabels(np.r_[df_['label2'], ['']], ha='center', fontsize=9)\n",
    "    ax3.yaxis.set_tick_params(pad=125)\n",
    "    \n",
    "    [label.set_fontweight('bold') if 'Pool' in label.get_text() else None for label in ax1.get_yticklabels()]\n",
    "    [label2.set_fontweight('bold') if 'Pool' in label.get_text() else None for label, label2 in zip(ax1.get_yticklabels(),ax2.get_yticklabels())]\n",
    "    \n",
    "    [label.set_color('k') if 'Pool' in label.get_text() else None for label in ax1.get_yticklabels()]\n",
    "    [label2.set_color('k') if 'Pool' in label.get_text() else None for label, label2 in zip(ax1.get_yticklabels(),ax2.get_yticklabels())]\n",
    "    \n",
    "    [label.set_color(clr_chn_) if 'China' in label.get_text() else None for label in ax1.get_yticklabels()]\n",
    "    [label2.set_color(clr_chn_) if 'China' in label.get_text() else None for label, label2 in zip(ax1.get_yticklabels(),ax2.get_yticklabels())]\n",
    "    \n",
    "    plt.text(.44, 1.07, 'Country', horizontalalignment='left', fontsize=10)\n",
    "    plt.text(3.59, 1.07, 'Mean (95% CrI)', horizontalalignment='right', fontsize=10)\n",
    "    plt.text(3.89, 1.05, 'Period\\n(m/d in 2022)', horizontalalignment='center', fontsize=10)\n",
    "    \n",
    "    plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "    if save_figures:\n",
    "        plt.savefig(f\"../../figures/fig-R0_meta_only_sympt_Tupper-{Tupper_}_Miura2023.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "    \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
